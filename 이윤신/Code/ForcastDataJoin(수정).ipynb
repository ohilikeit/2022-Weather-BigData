{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ForcastDataJoin(수정).ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1-PfzGZjK3HhdLkHGQdqXMGAxK3xEG0-Y","authorship_tag":"ABX9TyM4+959KAdW2+B8yzxJZmUb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","import pandas as pd\n","\n","func_list = ['mean', 'max']\n","\n","  # aws 지역명 매핑용 데이터\n","new_AWS = pd.read_csv('/content/drive/MyDrive/2022_WeatherContest_Data/DataSet/new_AWS_final.csv',encoding='cp949')\n","\n","  # 결측치 확인\n","def check_missing_col(dataframe):\n","    missing_col = []\n","    for col in dataframe.columns:\n","        missing_values = sum(dataframe[col].isna())\n","        is_missing = True if missing_values >= 1 else False\n","        if is_missing:\n","            print(f'결측치가 있는 컬럼은: {col} 입니다')\n","            print(f'해당 컬럼에 총 {missing_values} 개의 결측치가 존재합니다.')\n","            missing_col.append([col, dataframe[col].dtype])\n","    if missing_col == []:\n","        print('결측치가 존재하지 않습니다')\n","    return missing_col\n","\n","  # t4~t8 컬럼 삭제\n","def drop_col_t4_t8(df):\n","  drop_col = ['t4',\t't5',\t't6',\t't7',\t't8']\n","  if drop_col[0] in df.columns:\n","    df.drop(drop_col, axis=1, inplace=True)\n","  return df\n","\n","  # t1,t3~t8 컬럼 삭제\n","def drop_col_t1_t8(df):\n","  drop_col = ['t1','t3','t4',\t't5',\t't6',\t't7',\t't8']\n","  if drop_col[0] in df.columns:\n","    df.drop(drop_col, axis=1, inplace=True)\n","  return df\n","\n","  # area 매핑\n","def mapping_aws(df):\n","    df = pd.merge(df, new_AWS, on='stn_id', how='left')\n","    df.drop(['stn_id','지점명(한글)'], axis=1, inplace=True)\n","    return df\n","\n","  # 발표시각(tm_fc) 관측시간 오전 6시만 살림(관측 년,월,일은 살림)\n","def remove_hour(df):\n","  df = df.loc[df['tm_fc'].str.contains('06:00:00')]\n","  df['tm_fc'] = df.tm_fc.str.split().str[0]\n","  return df\n","\n","  # 지역명, 발표시간 기준으로 그룹화(최고값, 평균값)\n","def groupby_tm_area(df,start,end,group_key):\n","  df.fillna({'t1':0,'t2':0,'t3':0},inplace=True)\n","  cols = df.columns[start:end].tolist()\n","  df = df.groupby(group_key)[cols].agg(func_list).reset_index()\n","  df.columns = list(map('_'.join, df.columns.values))\n","  df = df.rename(columns={'tm_fc_' : 'tm_fc','area_' : 'area'})\n","  return df\n","\n","  # 지역명, 발표시간 기준으로 그룹화(최고값, 평균값)\n","def groupby_tm_area_07(df,start,end,group_key):\n","  df.fillna({'t2':0},inplace=True)\n","  cols = df.columns[start:end].tolist()\n","  df = df.groupby(group_key)[cols].agg(func_list).reset_index()\n","  df.columns = list(map('_'.join, df.columns.values))\n","  df = df.rename(columns={'tm_fc_' : 'tm_fc','area_' : 'area'})\n","  return df\n","\n","  # 예측 column 옆에 코드 각인 ( ex) A03_t2_max )\n","def engrave_code(df, cod):\n","  change_names = ['t1_mean','t1_max','t2_mean','t2_max','t3_mean','t3_max']\n","  for name in change_names:\n","    df.rename(columns={str(name):str(cod)+'_'+str(name)},inplace=True)\n","  return df\n","\n","    # 예측 column 옆에 코드 각인 ( ex) A03_t2_max )\n","def engrave_code_07(df, cod):\n","  change_names = ['t2_mean','t2_max']\n","  for name in change_names:\n","    df.rename(columns={str(name):str(cod)+'_'+str(name)},inplace=True)\n","  return df\n","\n","  # 발표시각(tm_fc) 형변환\n","def cast_tm_fc(df):\n","  df.tm_fc =pd.to_datetime(df.tm_fc)\n","  df.tm_fc = df.tm_fc.dt.strftime('%Y%m%d')\n","  return df\n","\n","  #t2_,t3_ 의 값을 각각 아래로 1칸,2칸 씩 이동\n","def match_forecast_data(df,start,end):\n","  if len(df) > 2:\n","    mid = (start+end)//2\n","    df.iloc[:,start:mid] = df.iloc[:,start:mid].shift(1)\n","    df.iloc[:,mid:end] = df.iloc[:,mid:end].shift(2)\n","  return df\n","\n","\n","  # 정렬 후 아래 값으로 결측치 채우기\n","def sort_fill(df, sort_key):\n","  df.sort_values(by=sort_key,inplace=True,na_position='last',ignore_index=True)\n","  df.fillna(method= 'bfill',inplace=True)\n","  return df\n","\n","  # fct 데이터 지역명 변경\n","def area_words(word):\n","    if (str(type(word))!=\"<class 'str'>\"):\n","      return word\n","\n","    if word in ['경상북도', '충청북도', '충청남도', '경상남도', '전라북도', '전라남도']:\n","        word = list(word)[0] + list(word)[2]\n","    elif word == '이어도':\n","        word = '제주'\n","    else:\n","        word = list(word)[0] + list(word)[1]\n","    return word \n","\n","\n","# aws 불러오기 및 전처리\n","path = '/content/drive/MyDrive/2022_WeatherContest_Data/DataSet/fct_life/mapping_aws/'\n","file_list = os.listdir(path)\n","\n","file_name_comm = 'fct_life_'\n","year_for_A03 = year_for_A04 = year_for_A05 = year_for_A06 = year_for_A07 = 0\n","\n","data_aws_A03 = []\n","data_aws_A04 = []\n","data_aws_A05 = []\n","data_aws_A06 = []\n","data_aws_A07 = []\n","\n","res_aws_data = [pd.DataFrame() for _ in range(5)]\n","\n","for i in file_list:\n","  if 'A03' in i:\n","    target = 'A03'\n","    res = pd.read_csv(path + file_name_comm+str(2012+year_for_A03)+'_'+target+'_aws.csv',encoding='cp949',index_col=0)\n","    data_aws_A03.append(res)\n","    res_aws_data[0] = pd.concat(data_aws_A03, axis=0, ignore_index = True)\n","    year_for_A03+=1\n","  \n","  elif 'A04' in i:\n","    target = 'A04'\n","    res = pd.read_csv(path + file_name_comm+str(2012+year_for_A04)+'_'+target+'_aws.csv',encoding='cp949',index_col=0)\n","    data_aws_A04.append(res)\n","    res_aws_data[1] = pd.concat(data_aws_A04, axis=0, ignore_index = True)\n","    year_for_A04+=1\n","\n","  elif 'A05' in i:\n","    target = 'A05'\n","    res = pd.read_csv(path + file_name_comm+str(2012+year_for_A05)+'_'+target+'_aws.csv',encoding='cp949',index_col=0)\n","    data_aws_A05.append(res)\n","    res_aws_data[2] = pd.concat(data_aws_A05, axis=0, ignore_index = True)\n","    year_for_A05+=1\n","\n","  elif 'A06' in i:\n","    target = 'A06'\n","    res = pd.read_csv(path + file_name_comm+str(2012+year_for_A06)+'_'+target+'_aws.csv',encoding='cp949',index_col=0)\n","    data_aws_A06.append(res)\n","    res_aws_data[3] = pd.concat(data_aws_A06, axis=0, ignore_index = True)\n","    year_for_A06+=1\n","\n","  elif 'A07' in i:\n","    target = 'A07'\n","    res = pd.read_csv(path + file_name_comm+str(2012+year_for_A07)+'_'+target+'_aws.csv',encoding='cp949',index_col=0)\n","    data_aws_A07.append(res)\n","    res_aws_data[4] = pd.concat(data_aws_A07, axis=0, ignore_index = True)\n","    year_for_A07+=1\n","\n","for i in range(len(res_aws_data)-1):\n","  res_aws_data[i] =drop_col_t4_t8(res_aws_data[i].copy())\n","  res_aws_data[i] = mapping_aws(res_aws_data[i].copy())\n","  res_aws_data[i] = remove_hour(res_aws_data[i].copy())\n","  res_aws_data[i] = groupby_tm_area(res_aws_data[i].copy(),2,5,['tm_fc','area'])\n","  code = 'A0'+str(i+3)\n","  res_aws_data[i] = engrave_code(res_aws_data[i].copy(),code)\n","  res_aws_data[i] = cast_tm_fc(res_aws_data[i].copy())\n","\n","res_aws_data[4] = drop_col_t1_t8(res_aws_data[4].copy())\n","res_aws_data[4] = mapping_aws(res_aws_data[4].copy())\n","res_aws_data[4] = remove_hour(res_aws_data[4].copy())\n","res_aws_data[4] = groupby_tm_area_07(res_aws_data[4].copy(),2,3,['tm_fc','area'])\n","code = 'A07'\n","res_aws_data[4] = engrave_code_07(res_aws_data[4].copy(),code)\n","res_aws_data[4] = cast_tm_fc(res_aws_data[4].copy())\n","\n","\n","\n","\n","# fct 전처리\n","def fct_preprocess(df,code):\n","  df=drop_col_t4_t8(df)\n","  df=remove_hour(df.copy())\n","  df = df.rename(columns={'1단계' : 'area'})\n","  df['area'] = [area_words(i) for i in df['area']]\n","  df.dropna(subset=['area'])\n","  df=groupby_tm_area(df,3,6,['tm_fc','area'])\n","  df=engrave_code(df, code)\n","  df = cast_tm_fc(df)\n","  \n","  return df\n","\n","# fct 07 전용 전처리 과정\n","def fct_preprocess_07(df,code):\n","  df=drop_col_t1_t8(df)\n","  df=remove_hour(df.copy())\n","  df = df.rename(columns={'1단계' : 'area'})\n","  df['area'] = [area_words(i) for i in df['area']]\n","  df.dropna(subset=['area'])\n","  df=groupby_tm_area_07(df,3,4,['tm_fc','area'])\n","  df=engrave_code_07(df, code)\n","  df = cast_tm_fc(df)\n","  \n","  return df\n","\n","path = '/content/drive/MyDrive/2022_WeatherContest_Data/DataSet/fct_life/mapping_fct/'\n","\n","res_fct_data = [pd.DataFrame() for _ in range(5)]\n","data_list = []\n","\n","# A03\n","fct_A03_2012 = pd.read_csv(path+'fct_life_2012_A03_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A03_2012 = fct_preprocess(fct_A03_2012,'A03')\n","data_list.append(fct_A03_2012)\n","fct_A03_2013 = pd.read_csv(path+'fct_life_2013_A03_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A03_2013 =fct_preprocess(fct_A03_2013,'A03')\n","data_list.append(fct_A03_2013)\n","fct_A03_2014 = pd.read_csv(path+'fct_life_2014_A03_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A03_2014 =fct_preprocess(fct_A03_2014,'A03')\n","data_list.append(fct_A03_2014)\n","fct_A03_2015 = pd.read_csv(path+'fct_life_2015_A03_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A03_2015 = fct_preprocess(fct_A03_2015,'A03')\n","data_list.append(fct_A03_2015)\n","fct_A03_2016 = pd.read_csv(path+'fct_life_2016_A03_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A03_2016 = fct_preprocess(fct_A03_2016,'A03')\n","data_list.append(fct_A03_2016)\n","res_fct_data[0] = pd.concat(data_list, axis=0, ignore_index = True)\n","del fct_A03_2012,fct_A03_2013,fct_A03_2014,fct_A03_2015,fct_A03_2016\n","\n","data_list = []\n","\n","#A04\n","fct_A04_2012 = pd.read_csv(path+'fct_life_2012_A04_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A04_2012 = fct_preprocess(fct_A04_2012,'A04')\n","data_list.append(fct_A04_2012)\n","fct_A04_2013 = pd.read_csv(path+'fct_life_2013_A04_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A04_2013 = fct_preprocess(fct_A04_2013,'A04')\n","data_list.append(fct_A04_2013)\n","fct_A04_2014 = pd.read_csv(path+'fct_life_2014_A04_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A04_2014 = fct_preprocess(fct_A04_2014,'A04')\n","data_list.append(fct_A04_2014)\n","fct_A04_2015 = pd.read_csv(path+'fct_life_2015_A04_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A04_2015 = fct_preprocess(fct_A04_2015,'A04')\n","data_list.append(fct_A04_2015)\n","fct_A04_2016 = pd.read_csv(path+'fct_life_2016_A04_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A04_2016 = fct_preprocess(fct_A04_2016,'A04')\n","data_list.append(fct_A04_2016)\n","res_fct_data[1] = pd.concat(data_list, axis=0, ignore_index = True)\n","del fct_A04_2012,fct_A04_2013,fct_A04_2014,fct_A04_2015,fct_A04_2016\n","\n","data_list = []\n","\n","#A05\n","fct_A05_2012 = pd.read_csv(path+'fct_life_2012_A05_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A05_2012 = fct_preprocess(fct_A05_2012,'A05')\n","data_list.append(fct_A05_2012)\n","fct_A05_2013 = pd.read_csv(path+'fct_life_2013_A05_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A05_2013 = fct_preprocess(fct_A05_2013,'A05')\n","data_list.append(fct_A05_2013)\n","fct_A05_2014 = pd.read_csv(path+'fct_life_2014_A05_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A05_2014 = fct_preprocess(fct_A05_2014,'A05')\n","data_list.append(fct_A05_2014)\n","fct_A05_2015 = pd.read_csv(path+'fct_life_2015_A05_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A05_2015 = fct_preprocess(fct_A05_2015,'A05')\n","data_list.append(fct_A05_2015)\n","fct_A05_2016 = pd.read_csv(path+'fct_life_2016_A05_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A05_2016 = fct_preprocess(fct_A05_2016,'A05')\n","data_list.append(fct_A05_2016)\n","res_fct_data[2] = pd.concat(data_list, axis=0, ignore_index = True)\n","del fct_A05_2012,fct_A05_2013,fct_A05_2014,fct_A05_2015,fct_A05_2016\n","\n","data_list = []\n","\n","#A06\n","fct_A06_2012 = pd.read_csv(path+'fct_life_2012_A06_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A06_2012 = fct_preprocess(fct_A06_2012,'A06')\n","data_list.append(fct_A06_2012)\n","fct_A06_2013 = pd.read_csv(path+'fct_life_2013_A06_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A06_2013 = fct_preprocess(fct_A06_2013,'A06')\n","data_list.append(fct_A06_2013)\n","fct_A06_2014 = pd.read_csv(path+'fct_life_2014_A06_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A06_2014 = fct_preprocess(fct_A06_2014,'A06')\n","data_list.append(fct_A06_2014)\n","fct_A06_2015 = pd.read_csv(path+'fct_life_2015_A06_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A06_2015 = fct_preprocess(fct_A06_2015,'A06')\n","data_list.append(fct_A06_2015)\n","fct_A06_2016 = pd.read_csv(path+'fct_life_2016_A06_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A06_2016 = fct_preprocess(fct_A06_2016,'A06')\n","data_list.append(fct_A06_2016)\n","res_fct_data[3] = pd.concat(data_list, axis=0, ignore_index = True)\n","del fct_A06_2012,fct_A06_2013,fct_A06_2014,fct_A06_2015,fct_A06_2016\n","\n","data_list = []\n","\n","#A07\n","fct_A07_2012 = pd.read_csv(path+'fct_life_2012_A07_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A07_2012 = fct_preprocess_07(fct_A07_2012,'A07')\n","data_list.append(fct_A07_2012)\n","fct_A07_2013 = pd.read_csv(path+'fct_life_2013_A07_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A07_2013 = fct_preprocess_07(fct_A07_2013,'A07')\n","data_list.append(fct_A07_2013)\n","fct_A07_2014 = pd.read_csv(path+'fct_life_2014_A07_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A07_2014 = fct_preprocess_07(fct_A07_2014,'A07')\n","data_list.append(fct_A07_2014)\n","fct_A07_2015 = pd.read_csv(path+'fct_life_2015_A07_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A07_2015 = fct_preprocess_07(fct_A07_2015,'A07')\n","data_list.append(fct_A07_2015)\n","fct_A07_2016 = pd.read_csv(path+'fct_life_2016_A07_fct.csv',encoding='cp949',index_col=0,error_bad_lines=False)\n","fct_A07_2016 = fct_preprocess_07(fct_A07_2016,'A07')\n","data_list.append(fct_A07_2016)\n","res_fct_data[4] = pd.concat(data_list, axis=0, ignore_index = True)\n","del fct_A07_2012,fct_A07_2013,fct_A07_2014,fct_A07_2015,fct_A07_2016\n","\n","del data_list\n","\n","\n","# aws , fct 병합\n","res_data = [pd.DataFrame() for _ in range(5)]\n","\n","\n","for i in range(4):\n","  new_col_name = 'A0'+str(i+3)\n","\n","  # aws와 fct의 tm_fc와 area의 교집합을 구하고, t1_,t2_,t3_ 의 평균값을 구한다.\n","  intersected_df = pd.merge(res_aws_data[i], res_fct_data[i], on=['tm_fc','area'], how='inner')\n","  intersected_df[new_col_name+'_t1_mean']=intersected_df.iloc[:,[2,8]].mean(axis=1)\n","  intersected_df[new_col_name+'_t1_max']=intersected_df.iloc[:,[3,9]].mean(axis=1)\n","  intersected_df[new_col_name+'_t2_mean']=intersected_df.iloc[:,[4,10]].mean(axis=1)\n","  intersected_df[new_col_name+'_t2_max']=intersected_df.iloc[:,[5,11]].mean(axis=1)\n","  intersected_df[new_col_name+'_t3_mean']=intersected_df.iloc[:,[6,12]].mean(axis=1)\n","  intersected_df[new_col_name+'_t3_max']=intersected_df.iloc[:,[7,13]].mean(axis=1)\n","  intersected_df.drop(intersected_df.columns[2:14],axis = 1,inplace = True)\n","  # 차집합(aws - fct) 구하고, t1_,t2_,t3_을 유지한다.\n","  set_diff_df_1 = pd.concat([res_aws_data[i], res_fct_data[i], res_fct_data[i]]).drop_duplicates(subset=['tm_fc', 'area'],keep=False)\n","  # 차집합(fct - aws) 구하고, t1_,t2_,t3_을 유지한다.\n","  set_diff_df_2 = pd.concat([res_fct_data[i], res_aws_data[i], res_aws_data[i]]).drop_duplicates(subset=['tm_fc', 'area'],keep=False)\n","\n","  # intersected_df, set_diff_df_1, set_diff_df_2 병합\n","  res_data[i] = pd.concat([intersected_df, set_diff_df_1, set_diff_df_2],axis=0,ignore_index=True)\n","\n","\n","\n","# aws와 fct의 tm_fc와 area의 교집합을 구하고, t1_,t2_,t3_ 의 평균값을 구한다.\n","intersected_df = pd.merge(res_aws_data[4], res_fct_data[4], on=['tm_fc','area'], how='inner')\n","intersected_df['A07_t2_mean']=intersected_df.iloc[:,[2,4]].mean(axis=1)\n","intersected_df['A07_t2_max']=intersected_df.iloc[:,[3,5]].mean(axis=1)\n","intersected_df.drop(intersected_df.columns[2:6],axis = 1,inplace = True)\n","# 차집합(aws - fct) 구하고, t1_,t2_,t3_을 유지한다.\n","set_diff_df_1 = pd.concat([res_aws_data[4], res_fct_data[4], res_fct_data[4]]).drop_duplicates(subset=['tm_fc', 'area'],keep=False)\n","# 차집합(fct - aws) 구하고, t1_,t2_,t3_을 유지한다.\n","set_diff_df_2 = pd.concat([res_fct_data[4], res_aws_data[4], res_aws_data[4]]).drop_duplicates(subset=['tm_fc', 'area'],keep=False)\n","\n","res_data[4] = pd.concat([intersected_df, set_diff_df_1, set_diff_df_2],axis=0,ignore_index=True)\n","\n","\n","\n","\n","\n","# res_data 전체 outer join\n","res = pd.DataFrame(columns=['tm_fc','area'])\n","for i in range(5):\n","  res = pd.merge(res,res_data[i],on=['tm_fc',\t'area'],how='outer')\n","res.sort_values(by=['tm_fc','area'],inplace=True)\n","res = res.reset_index(drop=True)\n","res.rename(columns={'tm_fc':'yyyymmdd'},inplace=True)\n","res = res.astype({'yyyymmdd':int})\n","\n","final = pd.read_csv('/content/drive/MyDrive/2022_WeatherContest_Data/DataSet/final/final.csv',encoding='cp949')\n","\n","final_res = pd.merge(final,res,on=['yyyymmdd','area'],how='outer')\n","\n","# 결측치 보정\n","from sklearn.linear_model import LinearRegression\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","\n","lr = LinearRegression()\n","\n","  # 새로 추가된 데이터에 결측값을 99998789(안쓰일거 같은 값)로 채움\n","final_cpy = final_res.copy()\n","final_cpy = final_cpy.iloc[:,71:101]\n","final_cpy.fillna(99998789,inplace=True)\n","final_res.iloc[:,71:101] = final_cpy\n","\n","  # 성별 까지 학습시키기 위해서 칼럼 순서를 바꿈 (~ sex frequency ~  => ~ frequency sex ~)\n","col_1 = final_res.columns[:2].to_list()\n","col_2 = final_res.columns[4:].to_list()\n","new_col=col_1+['frequency','sex']+col_2\n","final_res=final_res[new_col]\n","\n","imp = IterativeImputer(estimator=lr,missing_values=99998789, verbose=0, min_value = 0, imputation_order='roman',random_state=42)\n","w1 = final_res.drop(['yyyymmdd', 'area','frequency'], axis=1)\n","w2 = final_res[['yyyymmdd', 'area','frequency']]\n","X = imp.fit_transform(w1)\n","final_res = pd.concat([w2,pd.DataFrame(X, columns = final_res.columns[3:])], axis=1)\n","\n","  # 칼럼 순서 원상태로 복구 (~ frequency sex ~  => ~ sex frequency ~)\n","col_1 = final_res.columns[:2].to_list()\n","col_2 = final_res.columns[4:].to_list()\n","new_col=col_1+['sex','frequency']+col_2\n","final_res=final_res[new_col]\n","\n","\n","\n","# # 지연효과 반영 (t2,t3 각각 1칸 2칸 씩 내림)\n","# area_names = final_res['area'].unique()\n","\n","# alt_df = pd.DataFrame()\n","# for area in area_names:\n","#   for sex in [1,2]:\n","#     tmp_df = final_res[(final_res['area'] == area)&(final_res['sex'] == sex)]\n","#     tmp_df = match_forecast_data(tmp_df,73,77)\n","#     tmp_df = match_forecast_data(tmp_df,79,83)\n","#     tmp_df = match_forecast_data(tmp_df,85,89)\n","#     tmp_df = match_forecast_data(tmp_df,91,95)\n","#     tmp_df = match_forecast_data(tmp_df,97,101)\n","#     sort_key = ['yyyymmdd','area','sex']\n","#     tmp_df = sort_fill(tmp_df, sort_key)\n","#     alt_df = pd.concat([alt_df,tmp_df],axis=0)\n","    \n","# # final_res = alt_df\n","# final_res.sort_values(['yyyymmdd','area','sex'],inplace=True)\n","# final_res.reset_index(drop=True,inplace=True)\n","\n","final_res.drop(['tot_person','num_risk_age','year', 'month', 'day', 'day_differ', 'month_differ', 'avg_tca_mean', 'avg_tca_max',\n"," 'sum_ss_hr_mean', 'sum_ss_hr_max', 'ssrate_mean', 'ssrate_max', 'avg_rhm_mean', 'avg_rhm_min', 'avg_rhm_max', 'min_rhm_mean', 'min_rhm_min', 'min_rhm_max', 'sum_rn_mean', 'sum_rn_min',\n"," 'sum_rn_max', 'mi10_max_rn_mean', 'mi10_max_rn_min', 'mi10_max_rn_max', 'hr1_max_rn_mean', 'hr1_max_rn_min', 'hr1_max_rn_max', \n"," 'hr6_max_rn_mean', 'hr6_max_rn_min', 'hr6_max_rn_max', 'avg_ws_mean', 'avg_ws_min', 'avg_ws_max', 'max_ws_mean', 'max_ws_min', 'max_ws_max',\n"," 'avg_min_tg_mean', 'avg_min_tg_min', 'avg_min_tg_max', 'min_tg_mean', 'min_tg_min', 'min_tg_max', 'avg_ta_mean', 'avg_ta_min', 'avg_ta_max', \n"," 'max_ta_mean', 'max_ta_min', 'max_ta_max', 'min_ta_mean', 'min_ta_min', 'min_ta_max', 'SO2_mean', 'SO2_min', 'SO2_max','CO_mean','CO_min',\n"," 'CO_max','O3_mean','O3_min','O3_max','NO2_mean','NO2_min','NO2_max','PM10_mean','PM10_min','PM10_max'],axis = 1 , inplace=True)\n","\n","final_res_round = final_res.copy()\n","final_res.iloc[:,4:] =  final_res.iloc[:,4:].astype('int64')\n","\n","# # 최종데이터\n","final_res.to_csv('/content/drive/MyDrive/2022_WeatherContest_Data/DataSet/final/final_res.csv',encoding='cp949',index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ANcgKCkOC9x","executionInfo":{"status":"ok","timestamp":1658971945752,"user_tz":-540,"elapsed":798222,"user":{"displayName":"이윤신","userId":"01754473525638776627"}},"outputId":"fe7b02e3-2161-4023-d79a-005bc9f940c7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n","\n","\n","  exec(code_obj, self.user_global_ns, self.user_ns)\n"]}]}]}